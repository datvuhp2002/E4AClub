import { useState, useRef, useEffect } from "react";

const useSpeechRecognition = (
  onResult?: (text: string) => void,
  onEvaluate?: (audioURL: string) => void
) => {
  const [text, setText] = useState("");
  const [isListening, setIsListening] = useState(false);
  const [audioURL, setAudioURL] = useState<string | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);
  const textRef = useRef("");

  const startListening = async () => {
    if (!window.SpeechRecognition && !window.webkitSpeechRecognition) {
      alert("TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ Speech Recognition!");
      return;
    }

    streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorderRef.current = new MediaRecorder(streamRef.current);
    audioChunksRef.current = [];

    mediaRecorderRef.current.ondataavailable = (event) => {
      audioChunksRef.current.push(event.data);
    };

    mediaRecorderRef.current.start();

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    recognitionRef.current.lang = "en-US";
    recognitionRef.current.interimResults = false;
    recognitionRef.current.continuous = true; // ðŸ‘ˆ giá»¯ nháº­n diá»‡n liÃªn tá»¥c

    recognitionRef.current.onresult = (event: any) => {
      const speechText = Array.from(event.results)
        .map((result: any) => result[0].transcript)
        .join(" ");
      textRef.current = speechText;
      setText(speechText);
    };

    recognitionRef.current.start();
    setIsListening(true);
  };

  const stopListening = () => {
    if (!isListening) return;

    setIsListening(false);

    // Stop recognition
    recognitionRef.current?.stop();

    // Stop recording
    mediaRecorderRef.current?.stop();

    // Stop audio stream
    streamRef.current?.getTracks().forEach((track) => track.stop());
    streamRef.current = null;

    // Táº¡o audioURL khi recording stop
    recognitionRef.current.onend = () => {
      const audioBlob = new Blob(audioChunksRef.current, { type: "audio/webm" });
      const url = URL.createObjectURL(audioBlob);
      setAudioURL(url);
      onEvaluate?.(url);
      onResult?.(textRef.current);
    };
  };

  const resetAudio = () => {
    setAudioURL(null);
    setText("");
  };

  return {
    text,
    isListening,
    startListening,
    stopListening,
    resetAudio,
    audioURL,
  };
};


export default useSpeechRecognition;
